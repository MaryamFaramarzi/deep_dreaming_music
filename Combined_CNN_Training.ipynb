{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Combined CNN Training.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMSXTzyVeZj1S7+fBIns6np",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smhall97/hallucinating_GANs/blob/main/Combined_CNN_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "K53sDTzDU5Cj"
      },
      "source": [
        "# @title Imports\n",
        "\n",
        "import os\n",
        "import glob\n",
        "\n",
        "import torch\n",
        "\n",
        "import time\n",
        "import copy\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "\n",
        "\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torch.autograd import Variable\n",
        "from torch.optim import lr_scheduler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "DELQCGVQVB1B"
      },
      "source": [
        "# @title Helper Functions\n",
        "\n",
        "def scale_minmax(X):\n",
        "\n",
        "    X_scaled = (X - X.min()) / (X.max() - X.min())\n",
        "\n",
        "    return X_scaled\n",
        "\n",
        "\n",
        "def pickle_loader_mel(file):\n",
        "  with open(file, 'rb') as f:\n",
        "      data = pickle.load(f)\n",
        "      data = np.transpose(data, axes=[1, 2, 0])\n",
        "      data = scale_minmax(data)\n",
        "\n",
        "      zeros = np.zeros(data.shape)\n",
        "\n",
        "      data = np.concatenate((data, data, data), axis=2)\n",
        "\n",
        "  return(data)\n",
        "\n",
        "\n",
        "def pickle_loader_stft(file):\n",
        "  with open(file, 'rb') as f:\n",
        "      data = pickle.load(f)\n",
        "      data = np.squeeze(data, axis=0)\n",
        "\n",
        "      #real and imaginary parts are scaled independently\n",
        "      data[:,:,0] = scale_minmax(data[:,:,0])\n",
        "      data[:,:,1] = scale_minmax(data[:,:,1])\n",
        "\n",
        "      zeros = np.zeros((data.shape[0],data.shape[1],1))\n",
        "      data = np.concatenate((data, zeros), axis=2)\n",
        "\n",
        "  return(data)\n",
        "\n",
        "\n",
        "def make_sets(classes, items_per_class, ratios):\n",
        "  \"\"\"\n",
        "  parameters:\n",
        "  classes: number of classes in dataset\n",
        "  items_per_class: elements per class (assumes that the dataset is balanced across classes)\n",
        "  ratios: list or array with ratios for each subset [ratio_trainining, ratio_validation, ratio_test]\n",
        "  \"\"\"\n",
        "\n",
        "  train_size = ratios[0] * items_per_class\n",
        "  val_size = ratios[1] * items_per_class\n",
        "  test_size = ratios[2] * items_per_class\n",
        "\n",
        "  test_ix, val_ix, train_ix = np.array([]),np.array([]),np.array([])\n",
        "\n",
        "  for i in range(classes): \n",
        "    class_ix = items_per_class * i\n",
        "    \n",
        "    train_ix = np.append(train_ix, np.arange(train_size) + class_ix)\n",
        "    val_ix = np.append(val_ix, np.arange(train_size, train_size + val_size) + class_ix)\n",
        "    test_ix = np.append(test_ix, np.arange(train_size + val_size, train_size + val_size + test_size) + class_ix)\n",
        "\n",
        "  return train_ix.astype(int), val_ix.astype(int), test_ix.astype(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "L0XhepPOW0VI"
      },
      "source": [
        "# @title Set device (GPU or CPU)\n",
        "# NMA code\n",
        "# inform the user if the notebook uses GPU or CPU.\n",
        "\n",
        "def set_device():\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "  if device != \"cuda\":\n",
        "    print(\"WARNING: For this notebook to perform best, \"\n",
        "        \"if possible, in the menu under `Runtime` -> \"\n",
        "        \"`Change runtime type.`  select `GPU` \")\n",
        "  else:\n",
        "    print(\"GPU is enabled in this notebook.\")\n",
        "\n",
        "  return device\n",
        "\n",
        "device = set_device()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdgBbct-Wjr1"
      },
      "source": [
        "mount_drive = False\n",
        "\n",
        "if mount_drive:\n",
        "  from google.colab import drive\n",
        "drive.mount('/content/drive') #it will ask you for a verification code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9QAN-w9VX0g"
      },
      "source": [
        "# Set up experiment variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kwTJL1OVWn9"
      },
      "source": [
        "data_type = \"stft\" # alternative options \"mel\"\n",
        "data_augmentations = \"True\" # if no augmentation \"False\"\n",
        "augment_type = \"shuffled\" # alternative augmentations: \n",
        "\n",
        "dummy_mode = False\n",
        "\n",
        "path = '/content/drive/MyDrive/HallucinatingGANs/Code/data/'\n",
        "\n",
        "transform = 'mel'\n",
        "\n",
        "n_fft = 1024\n",
        "n_mels = 128\n",
        "hop_length = 256 # smaller hop size leads to better reconstruction but takes longer to compute\n",
        "power = 2.0 # squared power spectrogram\n",
        "samplerate =  22050\n",
        "\n",
        "def get_cfg_transform(t):\n",
        "  if t == 'stft':\n",
        "    params = '{}_{}'.format(str(n_fft), str(hop_length))\n",
        "    pickle_loader = pickle_loader_stft\n",
        "  \n",
        "  elif t == 'mel':\n",
        "    params = '{}_{}_{}'.format(str(n_fft), str(hop_length), n_mels)\n",
        "    pickle_loader = pickle_loader_mel\n",
        "  return params, pickle_loader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "f2vQP1mtW5b6"
      },
      "source": [
        "# @title Load pretrained VGG\n",
        "\"\"\"\n",
        "code extracted from:\n",
        "https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html#convnet-as-fixed-feature-extractor\n",
        "https://pytorch.org/vision/stable/models.html\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "vgg16 = models.vgg16(pretrained=True)\n",
        "\n",
        "# Freeze the network except the last layer / unfreeze layers to allow finetuning\n",
        "for param in vgg16.parameters():\n",
        "    param.requires_grad = True # If True it will train\n",
        "\n",
        "# Parameters of newly constructed modules have requires_grad=True by default\n",
        "# Add on classifier\n",
        "\n",
        "vgg16.classifier[6] = nn.Sequential(\n",
        "                      nn.Linear(vgg16.classifier[3].in_features, 256),\n",
        "                      nn.ReLU(), \n",
        "                      nn.Linear(256, n_classes),                   \n",
        "                      nn.LogSoftmax(dim=1))\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that only parameters of final layer are being optimized as\n",
        "# opposed to before.\n",
        "optimizer_conv = optim.SGD(vgg16.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7yhYaDrW_Cg"
      },
      "source": [
        "# @title Train model function from PyTorch\n",
        "\n",
        "# Original code from this tutorial: https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n",
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "    train_acc_list, val_acc_list = [], []\n",
        "    # train_loss, validation_loss = [], []\n",
        "    for epoch in tqdm(range(num_epochs)):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "            num_examples = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                num_examples += inputs.size(0)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs.float())\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "            # Different to tutorial, hardcoded dataset size\n",
        "            # print(dataset_sizes) from above \n",
        "            \n",
        "            #if phase == 'train':\n",
        "            #    num_examples = len(train_loader) * len(next(iter(train_loader))[0]) \n",
        "            #else:\n",
        "            #    num_examples = len(val_loader) * len(next(iter(val_loader))[0])\n",
        "            print('number of examples in loader = ', num_examples)\n",
        "            print(f'RUNNING LOSS: {running_loss}, RUNNING CORRECTS: {running_corrects}')\n",
        "\n",
        "            epoch_loss = running_loss / num_examples\n",
        "            print()\n",
        "            epoch_acc = running_corrects.double() / num_examples\n",
        "            if phase == 'train':\n",
        "              train_acc_list.append(epoch_acc)\n",
        "            else:\n",
        "              val_acc_list.append(epoch_acc)\n",
        "          \n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    \n",
        "    # model = model.to('cuda')\n",
        "    return model, train_acc_list, val_acc_list"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}